{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074fdaa-edff-468a-970f-6f5f26e93d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42168a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = OllamaLLM(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:latest\",  # Correct parameter name is `model`\n",
    "    base_url=\"http://localhost:11434\",  # Base URL for the Ollama service\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97eaaf2-34b7-4770-9949-e1abc4ca5226",
   "metadata": {},
   "source": [
    "First we embed some artificial documents and index them in a basic in-memory vector store. We will use [OpenAI](/docs/integrations/providers/openai/) embeddings, but any LangChain vector store or embeddings model will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cbcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Get embeddings.\n",
    "embeddings = ollama_embedding\n",
    "\n",
    "texts = [\n",
    "    \"Basquetball is a great sport.\",\n",
    "    \"Fly me to the moon is one of my favourite songs.\",\n",
    "    \"The Celtics are my favourite team.\",\n",
    "    \"This is a document about the Boston Celtics\",\n",
    "    \"I simply love going to the movies\",\n",
    "    \"The Boston Celtics won the game by 20 points\",\n",
    "    \"This is just a random text.\",\n",
    "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "    \"L. Kornet is one of the best Celtics players.\",\n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a retriever\n",
    "retriever = InMemoryVectorStore.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.invoke(query)\n",
    "for doc in docs:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d031a-43fa-42f4-93c4-2ba52c3c3ee5",
   "metadata": {},
   "source": [
    "Note that documents are returned in descending order of relevance to the query. The `LongContextReorder` document transformer will implement the re-ordering described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f7a0d-74d1-44f3-949d-6e758aa9f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# Reorder the documents:\n",
    "# Less relevant document will be at the middle of the list and more\n",
    "# relevant elements at beginning / end.\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "for doc in reordered_docs:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2ef0c-c397-4d8d-8118-3f7acf86d241",
   "metadata": {},
   "source": [
    "Below, we show how to incorporate the re-ordered documents into a simple question-answering chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533c74e-21cb-4f21-b063-4c66754a4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ollama_llm\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given these texts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")\n",
    "\n",
    "# Create and invoke the chain:\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "response = chain.invoke({\"context\": reordered_docs, \"query\": query})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
