{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-text-splitters tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Various Methods in LangChain's Text Splitter\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text and Loader Setup\n",
    "txt_file_loader = TextLoader(\"../../00-example_data/state_of_the_union.txt\")\n",
    "txt_file_documents = txt_file_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(txt_file_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "s_text = text_splitter.split_documents(txt_file_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "web_loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "web_documents = web_loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(web_documents)\n",
    "\n",
    "\n",
    "print(f\"Generated {len(split_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Documents\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = [\"LangChain enables easier integration of LLMs.\", \"Text splitters enhance text processing.\"]\n",
    "metadatas = [{\"source\": \"example1\"}, {\"source\": \"example2\"}]\n",
    "created_docs = text_splitter.create_documents(texts, metadatas=metadatas)\n",
    "print(\"Created Documents:\", created_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From HuggingFace Tokenizer\n",
    "text = \"\"\"LangChain provides a suite of tools for building language model applications. \n",
    "Text splitting is a key preprocessing step for managing large texts.\"\"\"\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "hf_text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=huggingface_tokenizer, chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "hf_chunks_text = hf_text_splitter.split_text(text)\n",
    "print(\"HuggingFace Tokenizer Chunks from text:\", hf_chunks_text)\n",
    "\n",
    "hf_chunks_documents = hf_text_splitter.split_documents(txt_file_documents)\n",
    "print(\"HuggingFace Tokenizer Chunks from documents:\", hf_chunks_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hf_chunks_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hf_chunks_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Documents\n",
    "transformed_docs = text_splitter.transform_documents(txt_file_documents)\n",
    "print(\"Transformed Documents:\", len(transformed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Documents (Asynchronously)\n",
    "async def async_transform_documents_demo():\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    transformed_docs = await text_splitter.atransform_documents(txt_file_documents)\n",
    "    print(\"Transformed Documents (Async):\", len(transformed_docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
