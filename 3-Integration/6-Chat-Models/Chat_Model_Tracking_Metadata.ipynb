{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0,  # Deterministic responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "I hope that made you smile! Do you want to hear another one?\n",
      "Token Usage Metadata: {'input_tokens': 32, 'output_tokens': 33, 'total_tokens': 65}\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "query = \"Can you tell me a joke?\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke(query)\n",
    "\n",
    "# Display the response and token usage metadata\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Token Usage Metadata: {response.usage_metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nI hope that made you smile! Do you want to hear another one?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-01-28T11:47:06.5023776Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1794913600, 'load_duration': 1368905500, 'prompt_eval_count': 32, 'prompt_eval_duration': 155000000, 'eval_count': 33, 'eval_duration': 268000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-ada22a74-92d3-48eb-b901-acbe0a55f893-0', usage_metadata={'input_tokens': 32, 'output_tokens': 33, 'total_tokens': 65})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Here\n",
      "Chunk: 's\n",
      "Chunk:  one\n",
      "Chunk: :\n",
      "\n",
      "\n",
      "Chunk: Did\n",
      "Chunk:  you\n",
      "Chunk:  know\n",
      "Chunk:  that\n",
      "Chunk:  there\n",
      "Chunk:  is\n",
      "Chunk:  a\n",
      "Chunk:  species\n",
      "Chunk:  of\n",
      "Chunk:  jelly\n",
      "Chunk: fish\n",
      "Chunk:  that\n",
      "Chunk:  is\n",
      "Chunk:  immortal\n",
      "Chunk: ?\n",
      "Chunk:  The\n",
      "Chunk:  Tur\n",
      "Chunk: rit\n",
      "Chunk: opsis\n",
      "Chunk:  do\n",
      "Chunk: hr\n",
      "Chunk: n\n",
      "Chunk: ii\n",
      "Chunk: ,\n",
      "Chunk:  also\n",
      "Chunk:  known\n",
      "Chunk:  as\n",
      "Chunk:  the\n",
      "Chunk:  \"\n",
      "Chunk: imm\n",
      "Chunk: ortal\n",
      "Chunk:  jelly\n",
      "Chunk: fish\n",
      "Chunk: ,\"\n",
      "Chunk:  can\n",
      "Chunk:  transform\n",
      "Chunk:  its\n",
      "Chunk:  body\n",
      "Chunk:  into\n",
      "Chunk:  a\n",
      "Chunk:  younger\n",
      "Chunk:  state\n",
      "Chunk:  through\n",
      "Chunk:  a\n",
      "Chunk:  process\n",
      "Chunk:  called\n",
      "Chunk:  trans\n",
      "Chunk: different\n",
      "Chunk: iation\n",
      "Chunk: .\n",
      "Chunk:  This\n",
      "Chunk:  means\n",
      "Chunk:  it\n",
      "Chunk:  can\n",
      "Chunk:  essentially\n",
      "Chunk:  revert\n",
      "Chunk:  back\n",
      "Chunk:  to\n",
      "Chunk:  its\n",
      "Chunk:  pol\n",
      "Chunk: yp\n",
      "Chunk:  stage\n",
      "Chunk:  and\n",
      "Chunk:  grow\n",
      "Chunk:  back\n",
      "Chunk:  into\n",
      "Chunk:  an\n",
      "Chunk:  adult\n",
      "Chunk:  again\n",
      "Chunk: ,\n",
      "Chunk:  making\n",
      "Chunk:  it\n",
      "Chunk:  theoretically\n",
      "Chunk:  immortal\n",
      "Chunk: !\n",
      "\n",
      "\n",
      "Chunk: Isn\n",
      "Chunk: 't\n",
      "Chunk:  that\n",
      "Chunk:  mind\n",
      "Chunk: -b\n",
      "Chunk: low\n",
      "Chunk: ing\n",
      "Chunk: ?\n",
      "Chunk: \n",
      "Final Response: Here's one:\n",
      "\n",
      "Did you know that there is a species of jellyfish that is immortal? The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can transform its body into a younger state through a process called transdifferentiation. This means it can essentially revert back to its polyp stage and grow back into an adult again, making it theoretically immortal!\n",
      "\n",
      "Isn't that mind-blowing?\n",
      "Usage Metadata: {'input_tokens': 31, 'output_tokens': 88, 'total_tokens': 119}\n"
     ]
    }
   ],
   "source": [
    "# Stream response and aggregate token usage\n",
    "aggregate = None\n",
    "for chunk in llm.stream(\"Tell me a fun fact!\"):\n",
    "    print(f\"Chunk: {chunk.content}\")\n",
    "    aggregate = chunk if aggregate is None else aggregate + chunk\n",
    "\n",
    "# Display the final aggregated response and token usage\n",
    "print(f\"Final Response: {aggregate.content}\")\n",
    "print(f\"Usage Metadata: {aggregate.usage_metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: 32\n",
      "Output Tokens: 33\n",
      "Total Tokens: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {response.usage_metadata['input_tokens']}\")\n",
    "print(f\"Output Tokens: {response.usage_metadata['output_tokens']}\")\n",
    "print(f\"Total Tokens: {response.usage_metadata['total_tokens']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Metadata: {'model': 'llama3.2:latest', 'created_at': '2025-01-28T11:47:06.5023776Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1794913600, 'load_duration': 1368905500, 'prompt_eval_count': 32, 'prompt_eval_duration': 155000000, 'eval_count': 33, 'eval_duration': 268000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response Metadata: {response.response_metadata}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
