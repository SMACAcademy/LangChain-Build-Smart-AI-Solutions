{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the Ollama model\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0  # Ensures consistent outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Tell me a joke about cats'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a simple prompt template with a placeholder for the topic\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# Invoke the template with a variable\n",
    "formatted_prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a chat prompt with a system message and user input\n",
    "chat_prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "# Invoke the template with a specific topic\n",
    "formatted_chat_prompt = chat_prompt.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "# Print the structured chat messages\n",
    "print(formatted_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why did the cat join a band?\n",
      "\n",
      "Because it wanted to be the purr-cussionist! (get it?)\n"
     ]
    }
   ],
   "source": [
    "# Invoke the Ollama model with the formatted prompt\n",
    "response = llm.invoke(formatted_chat_prompt)\n",
    "\n",
    "# Print the AI-generated response\n",
    "print(\"Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a chat prompt template with a placeholder for messages\n",
    "chat_prompt_with_placeholder = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "# Insert a list of messages dynamically\n",
    "formatted_prompt_with_msgs = chat_prompt_with_placeholder.invoke(\n",
    "    {\"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"How are you?\")]}\n",
    ")\n",
    "\n",
    "# Print the structured messages\n",
    "print(formatted_prompt_with_msgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_alternative = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"placeholder\", \"{msgs}\")  # Using a placeholder instead of MessagesPlaceholder\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], optional_variables=['msgs'], input_types={'msgs': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001E62EF6A020>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'msgs': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), MessagesPlaceholder(variable_name='msgs', optional=True)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_alternative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
