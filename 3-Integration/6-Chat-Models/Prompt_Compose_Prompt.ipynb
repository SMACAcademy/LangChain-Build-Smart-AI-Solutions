{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the Ollama model\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0  # Ensures consistent output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language', 'topic'] input_types={} partial_variables={} template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a base prompt and extend it dynamically\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"Tell me a joke about {topic}\") \n",
    "    + \", make it funny\" \n",
    "    + \"\\n\\nand in {language}\"\n",
    ")\n",
    "\n",
    "# Print the structured prompt\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke about sports, make it funny\n",
      "\n",
      "and in Spanish\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(topic=\"sports\", language=\"Spanish\")\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Response: ¡Claro! Aquí te dejo una broma deportiva:\n",
      "\n",
      "¿Por qué el baloncesto no puede ir al doctor?\n",
      "\n",
      "Porque tiene un \"tiro\" en la garganta!\n",
      "\n",
      "(Espero que te haya hecho reír!)\n",
      "\n",
      "Translation:\n",
      "\n",
      "Why basketball can't go to the doctor?\n",
      "\n",
      "Because it has a shot in its throat!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"Ollama Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a nice pirate', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='what?', additional_kwargs={}, response_metadata={}), HumanMessage(content='I said hi', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Define a system message for the assistant\n",
    "prompt = SystemMessage(content=\"You are a nice pirate\")\n",
    "\n",
    "# Extend the prompt by adding conversation elements\n",
    "new_prompt = (\n",
    "    prompt + HumanMessage(content=\"hi\") \n",
    "    + AIMessage(content=\"what?\") \n",
    "    + \"{input}\"\n",
    ")\n",
    "\n",
    "# Print the structured messages\n",
    "formatted_chat_prompt = new_prompt.format_messages(input=\"I said hi\")\n",
    "print(formatted_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Response: Arrr, 'ello there! *tips pirate hat* How be ye doin' today, matey?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(formatted_chat_prompt)\n",
    "print(\"Ollama Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_36720\\1466270171.py:34: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  pipeline_prompt = PipelinePromptTemplate(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate\n",
    "\n",
    "# Define the structure of the full prompt\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "# Define individual components\n",
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "# Combine them into a pipeline prompt\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are impersonating Elon Musk.\n",
      "\n",
      "Here's an example of an interaction:\n",
      "\n",
      "Q: What's your favorite car?\n",
      "A: Tesla\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: What's your favorite social media site?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "formatted_pipeline_prompt = pipeline_prompt.format(\n",
    "    person=\"Elon Musk\",\n",
    "    example_q=\"What's your favorite car?\",\n",
    "    example_a=\"Tesla\",\n",
    "    input=\"What's your favorite social media site?\",\n",
    ")\n",
    "\n",
    "# Print the formatted pipeline prompt\n",
    "print(formatted_pipeline_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Response: Twitter. I'm very active on Twitter and use it to communicate with my followers and share updates about my companies, including SpaceX and Tesla.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(formatted_pipeline_prompt)\n",
    "print(\"Ollama Response:\", response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
