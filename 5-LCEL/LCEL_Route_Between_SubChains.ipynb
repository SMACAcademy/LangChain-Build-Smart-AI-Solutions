{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a classification chain to categorize queries\n",
    "classification_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either `LangChain`, `Anthropic`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOllama(model=\"llama3.2\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(classification_chain.invoke({\"question\": \"How do I call Anthropic?\"}))  # Expected: \"Anthropic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain sub-chain\n",
    "langchain_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in LangChain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOllama(model=\"llama3.2\")\n",
    ")\n",
    "\n",
    "# Anthropic sub-chain\n",
    "anthropic_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in Anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOllama(model=\"llama3.2\")\n",
    ")\n",
    "\n",
    "# General question sub-chain\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOllama(model=\"llama3.2\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Anthropic is a conversational AI tool that allows users to create interactive stories, dialogue-based games, and chatbots. Here\\'s a step-by-step guide on how to use Anthropic:\\n\\n**Creating a New Project**\\n\\n1. Go to the Anthropic website ([www.anthropic.ai](http://www.anthropic.ai)) and sign up for an account.\\n2. Click on the \"Create\" button to start a new project.\\n3. Choose a template or start from scratch. Templates can help you get started quickly, while starting from scratch gives you full creative control.\\n\\n**Building Your Story**\\n\\n1. Once you\\'ve chosen your template or started a new project, you\\'ll be taken to the editor. This is where you build your story.\\n2. Use the editor\\'s drag-and-drop interface to add characters, scenes, and dialogue. You can also use pre-made assets like backgrounds and props.\\n3. Write your dialogue by typing in the chat window. You can also import existing text files or paste code snippets.\\n\\n**Conversational Flow**\\n\\n1. To create a conversational flow, you\\'ll need to set up a series of conditions and actions. This is done using a visual editor that looks like a flowchart.\\n2. Each condition will determine which action to take next in the conversation. You can use variables to store data between conditions.\\n3. Use the \"Transition\" button to move from one condition to another.\\n\\n**Testing Your Story**\\n\\n1. Once you\\'ve built your story, click on the \"Test\" button to run it through the editor.\\n2. Interact with your characters and see how the conversation flows.\\n3. Make adjustments as needed to refine your story.\\n\\n**Publishing Your Story**\\n\\n1. When you\\'re happy with your story, click on the \"Publish\" button to make it live.\\n2. You can share your story with others through a unique link or embed code.\\n3. You can also set up Anthropic\\'s built-in analytics to track user engagement and behavior.\\n\\n**Tips and Tricks**\\n\\n* Use the \"Debug\" mode to step through your conversation flow and identify any errors.\\n* Experiment with different dialogue options and variables to create more nuanced conversations.\\n* Take advantage of Anthropic\\'s built-in features, such as conditional logic and variables, to create complex conversational flows.\\n\\nThat\\'s a basic overview of how to use Anthropic! If you have any specific questions or need further guidance, feel free to ask.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-02-02T03:09:44.8408016Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4275441200, 'load_duration': 20817400, 'prompt_eval_count': 42, 'prompt_eval_duration': 11000000, 'eval_count': 500, 'eval_duration': 4241000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-7d28d098-5494-4e2c-9d57-dc130e9e1303-0' usage_metadata={'input_tokens': 42, 'output_tokens': 500, 'total_tokens': 542}\n",
      "#############################\n",
      "content=\"As Harrison Chase told me, using LangChain typically involves several steps that help you integrate it into your project's workflow. Here are the general steps to get started:\\n\\n1. **Install LangChain**: First, you'll need to install LangChain by running `pip install langchain` in your terminal. This will install the main package.\\n\\n2. **Initialize a new project**: Next, create a new Python file or directory for your project and initialize it with `from langchain import LangChain; langchain = LangChain()`. This sets up the core LangChain instance.\\n\\n3. **Define your knowledge graph**: You'll need to define your knowledge graph by creating an object that holds all relevant information about your project, such as entities, concepts, or data. You can create a JSON file or another data structure for this purpose.\\n\\n4. **Create a model**: After setting up your knowledge graph, you'll need to create a model that connects it to the LangChain instance. The most common models are the `PromptTemplate` and `TextPrompt`, which allow you to generate responses based on user input.\\n\\n5. **Test and refine**: Test your project by using the `generate` method of the LangChain instance, passing in relevant inputs like text prompts or entity mentions. Refine your model as needed until it provides accurate results for your use case.\\n\\n6. **Integrate with tools and services**: Depending on your requirements, you might need to integrate LangChain with other tools or services, such as APIs, databases, or UI libraries. This can be done by extending the `PromptTemplate` class or using other provided interfaces.\\n\\n7. **Continuously update and improve**: As new data becomes available, it's essential to update your knowledge graph and model to keep your project accurate and effective.\\n\\nBy following these steps, you'll be able to integrate LangChain into your project and start leveraging its capabilities for natural language processing tasks.\" additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-02-02T03:09:48.4344477Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3514307100, 'load_duration': 20428700, 'prompt_eval_count': 62, 'prompt_eval_duration': 11000000, 'eval_count': 391, 'eval_duration': 3480000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-03a53bda-427b-465b-a97f-6d1f70c434f0-0' usage_metadata={'input_tokens': 62, 'output_tokens': 391, 'total_tokens': 453}\n",
      "#############################\n",
      "content='4' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-02-02T03:09:48.5549411Z', 'done': True, 'done_reason': 'stop', 'total_duration': 53432800, 'load_duration': 20299100, 'prompt_eval_count': 43, 'prompt_eval_duration': 11000000, 'eval_count': 2, 'eval_duration': 19000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-3878b7a2-1a91-423d-8094-cc2fb214e3ae-0' usage_metadata={'input_tokens': 43, 'output_tokens': 2, 'total_tokens': 45}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define routing logic\n",
    "def route(info):\n",
    "    if \"anthropic\" in info[\"topic\"].lower():\n",
    "        return anthropic_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain\n",
    "\n",
    "# Create a pipeline that classifies and routes the query\n",
    "full_chain = {\"topic\": classification_chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(route)\n",
    "\n",
    "# Example invocations\n",
    "print(full_chain.invoke({\"question\": \"How do I use Anthropic?\"}))  # Routes to Anthropic chain\n",
    "print(\"#############################\")\n",
    "print(full_chain.invoke({\"question\": \"How do I use LangChain?\"}))  # Routes to LangChain chain\n",
    "print(\"#############################\")\n",
    "print(full_chain.invoke({\"question\": \"What's 2 + 2?\"}))  # Routes to General chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As Harrison Chase told me, to get started with Anthropic, you first need to create an account on their website. Once you\\'re logged in, you\\'ll be guided through a series of steps to set up your organization and connect it to LangChain.\\n\\nNext, you\\'ll need to install the Anthropic CLI tool, which will allow you to interact with Anthropic\\'s APIs from the command line. This can be done using pip: `pip install anthropic`.\\n\\nAfter installation, you can use the Anthropic CLI to authenticate, retrieve your API key, and connect it to LangChain projects.\\n\\nTo do this, create a new file named `.anthropic.json` in your project directory and add your API key to it. Then, run the command `anthropic login --username YOUR_USERNAME --password YOUR_PASSWORD` (replace \"YOUR_USERNAME\" and \"YOUR_PASSWORD\" with your actual Anthropic credentials).\\n\\nThis will authenticate you with Anthropic\\'s APIs, which can then be used with LangChain. You can verify the connection by running a simple script using the Anthropic CLI to retrieve some data.\\n\\nFor example, you can use `anthropic list --project YOUR_PROJECT_ID` (replace \"YOUR_PROJECT_ID\" with your actual project ID) to list the projects in your Anthropic account.\\n\\nOnce you\\'re connected to Anthropic\\'s APIs, you can start integrating LangChain with Anthropic\\'s features, such as entity recognition or content classification.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-02-02T03:11:59.7084663Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2475770900, 'load_duration': 19858600, 'prompt_eval_count': 62, 'prompt_eval_duration': 13000000, 'eval_count': 290, 'eval_duration': 2441000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-b0a1f00e-456b-4f98-b2b9-bb93f37861c4-0' usage_metadata={'input_tokens': 62, 'output_tokens': 290, 'total_tokens': 352}\n",
      "#############################\n",
      "content=\"As Harrison Chase told me, to get started with LangChain, you'll need to first install it using pip or conda, depending on your Python environment. Once installed, you can initialize a new LangChain app by running `langchain app init`.\\n\\nFrom there, you'll be prompted to choose a project template and configure some basic settings. You can then explore the different components of LangChain, such as the Hub API, Workspaces, and Tasks.\\n\\nTo use LangChain for any specific task, like generating text or answering questions, you'll need to create a Workspace and add the relevant Task to it. For example, if you want to generate text using a language model, you can create a Workspace with a `TextGenerationTask` added to it.\\n\\nLangChain also provides a number of pre-built Workspaces that you can use out-of-the-box for tasks like chatbot development or content generation. These pre-built Workspaces come with pre-configured Tasks and settings, making it easier to get started quickly.\\n\\nAs Harrison Chase told me, LangChain is all about making AI-powered workflows easy to build and manage. By leveraging its robust components and APIs, you can focus on building and refining your projects without getting bogged down in the technical details.\" additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-02-02T03:12:01.9177531Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2135952300, 'load_duration': 20394000, 'prompt_eval_count': 62, 'prompt_eval_duration': 11000000, 'eval_count': 253, 'eval_duration': 2102000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-adeac065-9585-4ac3-b5d4-f82f6b40bb4f-0' usage_metadata={'input_tokens': 62, 'output_tokens': 253, 'total_tokens': 315}\n",
      "#############################\n",
      "content='The answer is 4.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-02-02T03:12:02.0724207Z', 'done': True, 'done_reason': 'stop', 'total_duration': 89831400, 'load_duration': 19619100, 'prompt_eval_count': 43, 'prompt_eval_duration': 11000000, 'eval_count': 7, 'eval_duration': 57000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-64cb60cb-4379-4ddd-be1a-60d7256b101c-0' usage_metadata={'input_tokens': 43, 'output_tokens': 7, 'total_tokens': 50}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Define the branch-based routing logic\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n",
    "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "    general_chain,  # Default chain\n",
    ")\n",
    "\n",
    "# Create a pipeline using `RunnableBranch`\n",
    "full_chain = {\"topic\": classification_chain, \"question\": lambda x: x[\"question\"]} | branch\n",
    "\n",
    "# Example invocations\n",
    "print(full_chain.invoke({\"question\": \"How do I use Anthropic?\"}))  # Routes to Anthropic chain\n",
    "print(\"#############################\")\n",
    "print(full_chain.invoke({\"question\": \"How do I use LangChain?\"}))  # Routes to LangChain chain\n",
    "print(\"#############################\")\n",
    "print(full_chain.invoke({\"question\": \"What's 2 + 2?\"}))  # Routes to General chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black hole is a region in space where the gravitational pull is so strong that nothing, including light, can escape. It's formed when a massive star collapses in on itself and its gravity becomes so intense that it warps the fabric of spacetime.\n",
      "\n",
      "Think of it like this: imagine spacetime as a trampoline. If you place a heavy object on it, it will warp and curve. A black hole is like a cosmic sinkhole where the gravitational pull is so strong that it creates an impenetrable boundary called the event horizon. Once something crosses the event horizon, it's trapped forever.\n",
      "\n",
      "Black holes have three main properties: mass, charge, and angular momentum. The mass of a black hole determines its strength of gravity, while its charge and angular momentum determine how it behaves in the presence of other matter and radiation.\n",
      "\n",
      "Now, that's a concise introduction to black holes!\n",
      "#############################\n",
      "A great question in the realm of quantum mechanics and mathematics!\n",
      "\n",
      "As a mathematician, I'd be happy to break down the concept of a path integral into manageable steps.\n",
      "\n",
      "**Step 1: Understanding the Basics**\n",
      "\n",
      "* A path integral is a mathematical formalism used to solve differential equations and calculate probabilities in quantum mechanics.\n",
      "* It's based on the idea that a physical system can be represented as a sum of all possible paths that a particle can take between two points in space.\n",
      "\n",
      "**Step 2: Defining the Path Integral Formulation**\n",
      "\n",
      "* The path integral formulation is a mathematical representation of the Schrödinger equation, which describes the time-evolution of a quantum system.\n",
      "* It involves integrating over all possible paths in configuration space (a multi-dimensional space representing the positions and momenta of particles) that connect two initial and final states.\n",
      "\n",
      "**Step 3: Mathematical Representation**\n",
      "\n",
      "* The path integral is mathematically represented as:\n",
      "\\[ \\langle x_f | e^{-i\\int_{x_0}^{x_f} \\mathcal{H}(q, \\dot{q}) dt} | x_0 \\rangle = \\int_{x_0}^{x_f} D q(t) e^{-iS[q]} dq \\]\n",
      "* Here:\n",
      " + \\( \\langle x_f | \\) and \\( | x_0 \\rangle \\) are the initial and final states.\n",
      " + \\( \\mathcal{H}(q, \\dot{q}) \\) is the Hamiltonian of the system.\n",
      " + \\( e^{-i\\int_{x_0}^{x_f} \\mathcal{H}(q, \\dot{q}) dt} \\) is the time-evolution operator.\n",
      " + \\( D q(t) \\) represents the path integral measure (a Gaussian distribution in configuration space).\n",
      " + \\( S[q] \\) is the action functional, which encodes the system's dynamics.\n",
      "\n",
      "**Step 4: Path Integral Interpretation**\n",
      "\n",
      "* The path integral can be interpreted as a sum over all possible paths in configuration space that connect the initial and final states.\n",
      "* Each term in the sum corresponds to a different path, with an amplitude proportional to \\( e^{-S[q]} \\), which reflects the probability of taking that particular path.\n",
      "\n",
      "**Step 5: Solution and Applications**\n",
      "\n",
      "* The solution to the Schrödinger equation is obtained by evaluating the path integral.\n",
      "* Path integrals have been successfully applied in various areas of physics, including quantum field theory, condensed matter physics, and particle physics.\n",
      "\n",
      "That's a brief overview of what a path integral is! Do you have any specific questions or would you like me to elaborate on any of these steps?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define subject-specific prompts\n",
    "physics_template = \"\"\"You are a physics professor. Explain concepts concisely.\n",
    "Here is a question: {query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a mathematician. Break down problems into steps.\n",
    "Here is a question: {query}\"\"\"\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Define function to choose best-matching prompt and return formatted string\n",
    "def prompt_router(input):\n",
    "    if isinstance(input, str):  # Ensure input is a dict\n",
    "        input = {\"query\": input}\n",
    "\n",
    "    query = input[\"query\"]  # Extract query string\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    \n",
    "    return most_similar.format(query=query)  # Ensure output is a string\n",
    "\n",
    "# Create chain\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: {\"query\": x})  # Convert raw input into dict\n",
    "    | RunnableLambda(prompt_router)  # Routes to correct prompt\n",
    "    | ChatOllama(model=\"llama3.2\")  # Processes query with Ollama\n",
    "    | StrOutputParser()  # Extracts output as a string\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(chain.invoke(\"What’s a black hole?\"))  # Routes to Physics chain\n",
    "print(\"#############################\")\n",
    "print(chain.invoke(\"What’s a path integral?\"))  # Routes to Math chain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
